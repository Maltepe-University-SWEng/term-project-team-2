import streamlit as st
import torch
import torch.nn.functional as F
import os
import time
from main import CharTokenizer, SimpleLanguageModel

class FikraGenerator:
    def __init__(self, model_path, tokenizer_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = None
        self.model = None
        self.load_model(model_path, tokenizer_path)
    
    def load_model(self, model_path, tokenizer_path):
        # Tokenizer'Ä± yÃ¼kle
        try:
            self.tokenizer = CharTokenizer.load(tokenizer_path)
            # Model'i yÃ¼kle
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model = SimpleLanguageModel(
                vocab_size=self.tokenizer.vocab_size,
                embedding_dim=64,
                hidden_dim=128
            )
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            return True
        except Exception as e:
            st.error(f"Model yÃ¼klenirken hata: {e}")
            return False
    
    def generate_fikra(self, prompt, max_length=100, temperature=0.5):
        """Verilen baÅŸlangÄ±Ã§la fÄ±kra Ã¼ret"""
        if not self.model or not self.tokenizer:
            return "Model yÃ¼klenemedi! LÃ¼tfen dosya yollarÄ±nÄ± kontrol edin."
        
        # Prompt'u uygun formata getir
        if not prompt.startswith("FIKRA:"):
            prompt = f"FIKRA: {prompt}"
        
        # Prompt'u tokenize et
        context = torch.tensor(self.tokenizer.encode(prompt)).unsqueeze(0).to(self.device)
        generated = list(context[0].cpu().numpy())
        
        # Metin Ã¼retimi
        with torch.no_grad():
            for _ in range(max_length):
                # Kontekst boyutunu sÄ±nÄ±rla
                if len(context[0]) > 150:
                    context = context[:, -150:]
                
                # Sonraki tokeni tahmin et
                outputs = self.model(context)
                next_token_logits = outputs[0, -1, :] / temperature
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1)
                
                # Yeni tokeni ekle
                context = torch.cat([context, next_token.unsqueeze(0)], dim=1)
                generated.append(next_token.item())
                
                # BitiÅŸ kontrolleri
                if "\n\n" in self.tokenizer.decode(generated[-10:]):
                    break
                
                # Nokta, Ã¼nlem, soru iÅŸareti sonrasÄ± bitiÅŸ kontrol et
                if len(generated) > 100 and next_token.item() in [self.tokenizer.char_to_idx.get('.'), 
                                                              self.tokenizer.char_to_idx.get('!'),
                                                              self.tokenizer.char_to_idx.get('?')]:
                    if torch.rand(1).item() < 0.3:  # %30 olasÄ±lÄ±kla bitir
                        break
        
        # Ãœretilen metni dÃ¶ndÃ¼r
        return self.tokenizer.decode(generated)

# Streamlit arayÃ¼zÃ¼
def main():
    st.set_page_config(
        page_title="TÃ¼rkÃ§e FÄ±kra Ãœreteci",
        page_icon="ğŸ˜„",
        layout="centered"
    )
    
    st.title("ğŸ¤£ TÃ¼rkÃ§e FÄ±kra Ãœreteci")
    st.markdown("""
    Bu uygulama, derin Ã¶ÄŸrenme kullanarak TÃ¼rkÃ§e fÄ±kralar Ã¼retir. 
    Bir fÄ±kra baÅŸlangÄ±cÄ± yazÄ±n, yapay zeka geri kalanÄ±nÄ± tamamlasÄ±n!
    """)
    
    # Model seÃ§imi
    model_options = {
        "Prompt-Story FÄ±kra Modeli": {
            "model": "./model_data/finetune3_model/best_model.pt",
            "tokenizer": "./model_data/finetune3_model/tokenizer.pkl"
        },
        "Genel FÄ±kra Modeli": {
            "model": "./model_data/joke_model/best_model.pt",
            "tokenizer": "./model_data/joke_model/tokenizer.pkl"
        },
        "Temel TÃ¼rkÃ§e Modeli": {
            "model": "./model_data/base_model/model_epoch_9.pt",
            "tokenizer": "./model_data/base_model/tokenizer.pkl"
        }
    }
    
    selected_model = st.selectbox(
        "KullanÄ±lacak modeli seÃ§in:",
        list(model_options.keys())
    )
    
    # SeÃ§ilen modele gÃ¶re dosya yollarÄ±nÄ± belirle
    model_path = model_options[selected_model]["model"]
    tokenizer_path = model_options[selected_model]["tokenizer"]
    
    # Ä°lk Ã§alÄ±ÅŸtÄ±rmada modeli yÃ¼kle
    if "generator" not in st.session_state:
        with st.spinner("Model yÃ¼kleniyor..."):
            st.session_state.generator = FikraGenerator(model_path, tokenizer_path)
    
    # Model deÄŸiÅŸtirildiÄŸinde yeniden yÃ¼kle
    if "last_model" not in st.session_state or st.session_state.last_model != selected_model:
        with st.spinner("Model deÄŸiÅŸtirildi, yeni model yÃ¼kleniyor..."):
            st.session_state.generator = FikraGenerator(model_path, tokenizer_path)
            st.session_state.last_model = selected_model
    
    # Parametre kontrolleri
    col1, col2 = st.columns(2)
    with col1:
        temperature = st.slider(
            "YaratÄ±cÄ±lÄ±k (Temperature):",
            min_value=0.1,
            max_value=1.5,
            value=0.5,
            step=0.1,
            help="DÃ¼ÅŸÃ¼k deÄŸerler daha tutarlÄ±, yÃ¼ksek deÄŸerler daha yaratÄ±cÄ± Ã§Ä±ktÄ±lar Ã¼retir"
        )
    
    with col2:
        max_length = st.slider(
            "Maksimum Uzunluk:",
            min_value=50,
            max_value=500,
            value=100,
            step=50,
            help="Ãœretilecek metnin maksimum karakter sayÄ±sÄ±"
        )
    
    # HazÄ±r baÅŸlangÄ±Ã§ Ã¶nerileri
    prompt_suggestions = [
        "Temel bir gÃ¼n",
        "Nasreddin Hoca",
        "Ä°ki arkadaÅŸ",
        "AdamÄ±n biri",
        "Ã–ÄŸretmen sÄ±nÄ±fta"
    ]
    
    selected_suggestion = st.selectbox(
        "HazÄ±r baÅŸlangÄ±Ã§ seÃ§in (veya aÅŸaÄŸÄ±ya kendi baÅŸlangÄ±cÄ±nÄ±zÄ± yazÄ±n):",
        [""] + prompt_suggestions
    )
    
    # Metin giriÅŸi
    user_prompt = st.text_input(
        "FÄ±kra baÅŸlangÄ±cÄ±nÄ±zÄ± buraya yazÄ±n:",
        value=selected_suggestion
    )
    
    # FÄ±kra Ã¼retme butonu
    if st.button("FÄ±kra Ãœret", type="primary", disabled=not user_prompt):
        if not user_prompt:
            st.warning("LÃ¼tfen bir fÄ±kra baÅŸlangÄ±cÄ± girin!")
        else:
            with st.spinner("FÄ±kra Ã¼retiliyor..."):
                # Daha uzun sÃ¼ren iÅŸlemlerde gÃ¶rsel geri bildirim iÃ§in
                progress_bar = st.progress(0)
                for i in range(100):
                    time.sleep(0.01)  # SimÃ¼le edilmiÅŸ hesaplama zamanÄ±
                    progress_bar.progress(i + 1)
                
                generated_fikra = st.session_state.generator.generate_fikra(
                    user_prompt, 
                    max_length=max_length,
                    temperature=temperature
                )
            
            # Sonucu gÃ¶ster
            st.success("FÄ±kra Ã¼retildi!")
            st.markdown("### Ãœretilen FÄ±kra")
            st.markdown(f"<div style='background-color: #1a237e; padding: 15px; border-radius: 5px;'>{generated_fikra}</div>", unsafe_allow_html=True)
            
            # Ãœretim parametrelerini gÃ¶ster
            st.info(f"Bu fÄ±kra temperature={temperature}, max_length={max_length} ile Ã¼retilmiÅŸtir.")
    
    # Alt bilgi
    st.markdown("---")
    st.markdown("""
    **Not:** Bu uygulama, LSTM tabanlÄ± dil modeli kullanarak fÄ±kra Ã¼retir. Ãœretilen iÃ§erik tamamen yapay zeka tarafÄ±ndan oluÅŸturulur.
    """)

if __name__ == "__main__":
    main()